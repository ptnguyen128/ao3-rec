{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "from unidecode import unidecode\n",
    "\n",
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_empty = False\n",
    "username = \"incessantbeat\"\n",
    "url = f'https://archiveofourown.org/users/{username}/bookmarks'\n",
    "page = 1\n",
    "num_requested_fic = 0\n",
    "num_recorded_fic = 0\n",
    "csv_name = \"\"\n",
    "multichap_only = False\n",
    "tags = []\n",
    "\n",
    "# keep track of all processed ids to avoid repeats:\n",
    "# this is separate from the temporary batch of ids\n",
    "# that are written to the csv and then forgotten\n",
    "seen_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main user's bookmarked ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids():\n",
    "    \"\"\" Get work ids of bookmarked fics \"\"\"\n",
    "    global page_empty\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, \"lxml\")\n",
    "\n",
    "    fics = soup.select(\"li.bookmark.blurb.group\")\n",
    "    # see if we've gone too far and run out of fic:\n",
    "    if len(fics) == 0:\n",
    "        page_empty = True\n",
    "        print(\"No more fics to fetch!\")\n",
    "        return\n",
    "\n",
    "    # process list for new fic ids\n",
    "    ids = []\n",
    "    for idx, f in enumerate(fics):\n",
    "        try:\n",
    "            header = f.find('h4', class_='heading').find(href=True)\n",
    "            t = header['href'].split('/')[-1]\n",
    "            n = header.text\n",
    "            if t not in seen_ids:\n",
    "                ids.append(t)\n",
    "                seen_ids.append(t)\n",
    "        except:\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "def update_url_to_next_page():\n",
    "    global url\n",
    "    global page\n",
    "    key = \"page=\"\n",
    "    start = url.find(key)\n",
    "\n",
    "    # there is already a page indicator in the url\n",
    "    if start != -1:\n",
    "        # find where in the url the page indicator starts and ends\n",
    "        page_start_index = start + len(key)\n",
    "        page_end_index = url.find(\"&\", page_start_index)\n",
    "        # if it's in the middle of the url\n",
    "        if page_end_index != -1:\n",
    "            page = int(url[page_start_index:page_end_index]) + 1\n",
    "            url = url[:page_start_index] + str(page) + url[page_end_index:]\n",
    "        # if it's at the end of the url\n",
    "        else:\n",
    "            page = int(url[page_start_index:]) + 1\n",
    "            url = url[:page_start_index] + str(page)\n",
    "\n",
    "    # there is no page indicator, so we are on page 1\n",
    "    else:\n",
    "        # there are other modifiers\n",
    "        if url.find(\"?\") != -1:\n",
    "            url = url + \"&page=2\"\n",
    "        # there are no modifiers yet\n",
    "        else:\n",
    "            url = url + \"?page=2\"\n",
    "        page = 2\n",
    "\n",
    "\n",
    "def retrieve_ids():\n",
    "    while not page_empty:\n",
    "        time.sleep(5)\n",
    "        print(f\"Processing page {page}...\")\n",
    "        ids = get_ids()\n",
    "        update_url_to_next_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1...\n",
      "Processing page 2...\n",
      "No more fics to fetch!\n",
      "['28429821', '33658459', '33494707', '29839875', '21332461', '25333126', '26027836', '29618487', '29288772', '25839787', '23586082', '24363613', '25681186', '23492518', '24260716', '12830118', '12520952']\n"
     ]
    }
   ],
   "source": [
    "retrieve_ids()\n",
    "print(seen_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: do I need to write bookmarked ids to a text file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata of bookmarked ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get author(s)\n",
    "def get_authors(meta):\n",
    "    tags = meta.contents\n",
    "    authors = []\n",
    "    for tag in tags:\n",
    "        if tag.name == 'a':\n",
    "            authors.append(tag.contents[0])\n",
    "    return authors\n",
    "\n",
    "def get_tag_info(category, meta):\n",
    "    '''\n",
    "    given a category and a 'work meta group, returns a list of tags (eg, 'rating' -> 'explicit')\n",
    "    '''\n",
    "    try:\n",
    "        tag_list = meta.find(\"dd\", class_=str(category) + ' tags').find_all(class_=\"tag\")\n",
    "    except AttributeError as e:\n",
    "        return []\n",
    "    return [unidecode(result.text).rstrip().lstrip().lower() for result in tag_list] \n",
    "\n",
    "def get_tags(meta):\n",
    "    '''\n",
    "    returns a list of lists, of\n",
    "    rating, category, fandom, pairing, characters, additional_tags\n",
    "    '''\n",
    "    tags = ['rating', 'category', 'fandom', 'relationship', 'character', 'freeform']\n",
    "    info_list = list(map(lambda tag: get_tag_info(tag, meta), tags))\n",
    "    res = {}\n",
    "    for tag, info in zip(tags, info_list):\n",
    "        res[tag] = info\n",
    "    return res\n",
    "\n",
    "def get_stats(meta):\n",
    "    categories = ['language', 'published', 'status', 'words', 'chapters', 'comments', 'kudos', 'bookmarks', 'hits']\n",
    "    stats = list(map(lambda category: meta.find(\"dd\", class_=category), categories))\n",
    "    res = {}\n",
    "    for cat, stat in zip(categories, stats):\n",
    "        if stat:\n",
    "            res[cat] = unidecode(stat.text).rstrip().lstrip().lower()\n",
    "        else:\n",
    "            res[cat] = np.nan\n",
    "    return res\n",
    "\n",
    "def get_kudos(meta):\n",
    "    if (meta):\n",
    "        users = []\n",
    "        ## hunt for kudos' contents\n",
    "        kudos = meta.contents\n",
    "\n",
    "        # extract user names\n",
    "        for kudo in kudos:\n",
    "            if kudo.name == 'a':\n",
    "                if 'more users' not in kudo.contents[0] and '(collapse)' not in kudo.contents[0]:\n",
    "                    users.append(kudo.contents[0])\n",
    "\n",
    "        return users\n",
    "    return []\n",
    "\n",
    "# get bookmarks by page\n",
    "def get_bookmarks(url):\n",
    "    bookmarks = []\n",
    "\n",
    "    req = requests.get(url)\n",
    "    src = req.text\n",
    "\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(src, 'html.parser')\n",
    "\n",
    "    print('scraping bookmarks ')\n",
    "\n",
    "    # find all pages\n",
    "    if (soup.find('ol', class_='pagination actions')):\n",
    "        pages = soup.find('ol', class_='pagination actions').findChildren(\"li\" , recursive=False)\n",
    "        max_pages = int(pages[-2].contents[0].contents[0])\n",
    "        count = 1\n",
    "\n",
    "        while count <= max_pages:\n",
    "            # extract each bookmark per user\n",
    "            tags = soup.findAll('h5', class_='byline heading')\n",
    "            bookmarks += get_other_users(tags)\n",
    "\n",
    "            # next page\n",
    "            count+=1\n",
    "            req = requests.get(url+'?page='+str(count))\n",
    "            src = req.text\n",
    "            soup = BeautifulSoup(src, 'html.parser')\n",
    "            time.sleep(5)\n",
    "    else:\n",
    "        tags = soup.findAll('h5', class_='byline heading')\n",
    "        bookmarks += get_other_users(tags)\n",
    "    return bookmarks\n",
    "\n",
    "# get users form bookmarks, excluding yourself\n",
    "def get_other_users (meta):\n",
    "    users = []\n",
    "    for tag in meta:\n",
    "            user = tag.findChildren(\"a\" , recursive=False)[0].contents[0]\n",
    "            users.append(user)\n",
    "    return [u for u in users if u != username]\n",
    "\n",
    "def access_denied(soup):\n",
    "    if soup.find(class_=\"flash error\"):\n",
    "        return True\n",
    "    if not soup.find(class_=\"work meta group\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_metadata(fic_id):\n",
    "    print(f\"Scraping {fic_id}...\")\n",
    "    url = f'http://archiveofourown.org/works/{fic_id}?view_adult=true'\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    if access_denied(soup):\n",
    "        print('Access Denied')\n",
    "        return\n",
    "    else:\n",
    "        meta = soup.find(\"dl\", class_=\"work meta group\")\n",
    "        meta_dict = {}\n",
    "        meta_dict[\"url\"] = url\n",
    "        authors = get_authors(soup.find(\"h3\", class_=\"byline heading\"))\n",
    "        # author column - string if one author\n",
    "        if len(authors)==1:\n",
    "            meta_dict[\"author\"] = authors[0]\n",
    "        else:\n",
    "            meta_dict[\"author\"] = authors\n",
    "        \n",
    "        # unpack tags and stats sub-tags\n",
    "        for key, value in {**get_tags(meta), **get_stats(meta)}.items():\n",
    "            if isinstance(value, list) and len(value) == 1:\n",
    "                meta_dict[key] = value[0]\n",
    "            else:\n",
    "                meta_dict[key] = value\n",
    "                \n",
    "        meta_dict[\"title\"] = unidecode(soup.find(\"h2\", class_=\"title heading\").string).strip()\n",
    "        visible_kudos = get_kudos(soup.find('p', class_='kudos'))\n",
    "        hidden_kudos = get_kudos(soup.find('span', class_='kudos_expanded hidden'))\n",
    "        meta_dict[\"all_kudos\"] = visible_kudos + hidden_kudos\n",
    "        \n",
    "        #get bookmarks\n",
    "        bookmark_url = f'http://archiveofourown.org/works/{fic_id}/bookmarks'\n",
    "        meta_dict[\"all_bookmarks\"] = get_bookmarks(bookmark_url)\n",
    "        return meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'http://archiveofourown.org/works/28429821?view_adult=true'\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "meta = soup.find(\"dl\", class_=\"work meta group\")\n",
    "dest = {**get_tags(meta), **get_stats(meta)} \n",
    "for key, value in dest.items():\n",
    "    print(value)\n",
    "    print(isinstance(value, list))\n",
    "    if isinstance(value, list):\n",
    "        print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 28429821...\n",
      "scraping bookmarks \n"
     ]
    }
   ],
   "source": [
    "d = get_metadata(28429821)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28429821\n",
      "Scraping 28429821...\n",
      "scraping bookmarks \n",
      "1 33658459\n",
      "Scraping 33658459...\n",
      "scraping bookmarks \n",
      "2 33494707\n",
      "Scraping 33494707...\n",
      "scraping bookmarks \n",
      "3 29839875\n",
      "Scraping 29839875...\n",
      "scraping bookmarks \n"
     ]
    }
   ],
   "source": [
    "for idx, fic_id in enumerate(seen_ids[:4]):\n",
    "    print(idx, fic_id)\n",
    "    if idx == 0:\n",
    "        df = pd.DataFrame([get_metadata(fic_id)])\n",
    "    else:\n",
    "        df = df.append([get_metadata(fic_id)], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>category</th>\n",
       "      <th>fandom</th>\n",
       "      <th>relationship</th>\n",
       "      <th>character</th>\n",
       "      <th>freeform</th>\n",
       "      <th>language</th>\n",
       "      <th>published</th>\n",
       "      <th>status</th>\n",
       "      <th>words</th>\n",
       "      <th>chapters</th>\n",
       "      <th>comments</th>\n",
       "      <th>kudos</th>\n",
       "      <th>bookmarks</th>\n",
       "      <th>hits</th>\n",
       "      <th>title</th>\n",
       "      <th>all_kudos</th>\n",
       "      <th>all_bookmarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://archiveofourown.org/works/28429821?view...</td>\n",
       "      <td>tinymark (lumoon33)</td>\n",
       "      <td>mature</td>\n",
       "      <td>m/m</td>\n",
       "      <td>nct (band)</td>\n",
       "      <td>lee donghyuck | haechan/mark lee</td>\n",
       "      <td>[mark lee (nct), lee donghyuck | haechan, na j...</td>\n",
       "      <td>[alternate universe - college/university, frie...</td>\n",
       "      <td>english</td>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4199</td>\n",
       "      <td>1/1</td>\n",
       "      <td>32</td>\n",
       "      <td>369</td>\n",
       "      <td>42</td>\n",
       "      <td>4669</td>\n",
       "      <td>falling to the bathroom floor</td>\n",
       "      <td>[otterseoul, ari_thereyet, Slut_Seokjinnie, sk...</td>\n",
       "      <td>[chenlecentric, leehaechanace, beom00, lovedia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://archiveofourown.org/works/33658459?view...</td>\n",
       "      <td>MitchMatchedSocks</td>\n",
       "      <td>explicit</td>\n",
       "      <td>m/m</td>\n",
       "      <td>nct (band)</td>\n",
       "      <td>jung yoonoh | jaehyun/suh youngho | johnny</td>\n",
       "      <td>[jung yoonoh | jaehyun, suh youngho | johnny]</td>\n",
       "      <td>[alternate universe - college/university, oral...</td>\n",
       "      <td>english</td>\n",
       "      <td>2021-09-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6346</td>\n",
       "      <td>1/1</td>\n",
       "      <td>64</td>\n",
       "      <td>941</td>\n",
       "      <td>178</td>\n",
       "      <td>7480</td>\n",
       "      <td>Fixate On This</td>\n",
       "      <td>[KGP425, neodump, Marchingtodrums, jjstyle5, s...</td>\n",
       "      <td>[KGP425, neodump, h_dery99, jjstyle5, softgayj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://archiveofourown.org/works/33494707?view...</td>\n",
       "      <td>yutamatic</td>\n",
       "      <td>explicit</td>\n",
       "      <td>m/m</td>\n",
       "      <td>nct (band)</td>\n",
       "      <td>lee donghyuck | haechan/mark lee</td>\n",
       "      <td>[mark lee (nct), lee donghyuck | haechan]</td>\n",
       "      <td>[friends with benefits, recreational drug use,...</td>\n",
       "      <td>english</td>\n",
       "      <td>2021-09-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2752</td>\n",
       "      <td>1/1</td>\n",
       "      <td>17</td>\n",
       "      <td>271</td>\n",
       "      <td>30</td>\n",
       "      <td>2845</td>\n",
       "      <td>tell me how love will ruin us</td>\n",
       "      <td>[Dead0Scream, sibag, Wo0o0o, HyeSeong21, lucky...</td>\n",
       "      <td>[taekookisreall, J10W, mkImain, diamondacequee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://archiveofourown.org/works/29839875?view...</td>\n",
       "      <td>beneathyourbravery</td>\n",
       "      <td>explicit</td>\n",
       "      <td>m/m</td>\n",
       "      <td>nct (band)</td>\n",
       "      <td>[lee donghyuck | haechan/mark lee, minor or ba...</td>\n",
       "      <td>[mark lee (nct), lee donghyuck | haechan, suh ...</td>\n",
       "      <td>[alternate universe, angst, slow burn, poetry,...</td>\n",
       "      <td>english</td>\n",
       "      <td>2021-03-04</td>\n",
       "      <td>2021-03-04</td>\n",
       "      <td>42149</td>\n",
       "      <td>2/2</td>\n",
       "      <td>75</td>\n",
       "      <td>425</td>\n",
       "      <td>100</td>\n",
       "      <td>6684</td>\n",
       "      <td>the fire kiss of a seraphim</td>\n",
       "      <td>[hanasuminu, ilyjh97, fullsunflower0606, dande...</td>\n",
       "      <td>[subancha, stillstuckonu0, markeucry, renjunlu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url               author  \\\n",
       "0  http://archiveofourown.org/works/28429821?view...  tinymark (lumoon33)   \n",
       "1  http://archiveofourown.org/works/33658459?view...    MitchMatchedSocks   \n",
       "2  http://archiveofourown.org/works/33494707?view...            yutamatic   \n",
       "3  http://archiveofourown.org/works/29839875?view...   beneathyourbravery   \n",
       "\n",
       "     rating category      fandom  \\\n",
       "0    mature      m/m  nct (band)   \n",
       "1  explicit      m/m  nct (band)   \n",
       "2  explicit      m/m  nct (band)   \n",
       "3  explicit      m/m  nct (band)   \n",
       "\n",
       "                                        relationship  \\\n",
       "0                   lee donghyuck | haechan/mark lee   \n",
       "1         jung yoonoh | jaehyun/suh youngho | johnny   \n",
       "2                   lee donghyuck | haechan/mark lee   \n",
       "3  [lee donghyuck | haechan/mark lee, minor or ba...   \n",
       "\n",
       "                                           character  \\\n",
       "0  [mark lee (nct), lee donghyuck | haechan, na j...   \n",
       "1      [jung yoonoh | jaehyun, suh youngho | johnny]   \n",
       "2          [mark lee (nct), lee donghyuck | haechan]   \n",
       "3  [mark lee (nct), lee donghyuck | haechan, suh ...   \n",
       "\n",
       "                                            freeform language   published  \\\n",
       "0  [alternate universe - college/university, frie...  english  2020-12-30   \n",
       "1  [alternate universe - college/university, oral...  english  2021-09-03   \n",
       "2  [friends with benefits, recreational drug use,...  english  2021-09-09   \n",
       "3  [alternate universe, angst, slow burn, poetry,...  english  2021-03-04   \n",
       "\n",
       "       status  words chapters comments kudos bookmarks  hits  \\\n",
       "0         NaN   4199      1/1       32   369        42  4669   \n",
       "1         NaN   6346      1/1       64   941       178  7480   \n",
       "2         NaN   2752      1/1       17   271        30  2845   \n",
       "3  2021-03-04  42149      2/2       75   425       100  6684   \n",
       "\n",
       "                           title  \\\n",
       "0  falling to the bathroom floor   \n",
       "1                 Fixate On This   \n",
       "2  tell me how love will ruin us   \n",
       "3    the fire kiss of a seraphim   \n",
       "\n",
       "                                           all_kudos  \\\n",
       "0  [otterseoul, ari_thereyet, Slut_Seokjinnie, sk...   \n",
       "1  [KGP425, neodump, Marchingtodrums, jjstyle5, s...   \n",
       "2  [Dead0Scream, sibag, Wo0o0o, HyeSeong21, lucky...   \n",
       "3  [hanasuminu, ilyjh97, fullsunflower0606, dande...   \n",
       "\n",
       "                                       all_bookmarks  \n",
       "0  [chenlecentric, leehaechanace, beom00, lovedia...  \n",
       "1  [KGP425, neodump, h_dery99, jjstyle5, softgayj...  \n",
       "2  [taekookisreall, J10W, mkImain, diamondacequee...  \n",
       "3  [subancha, stillstuckonu0, markeucry, renjunlu...  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
