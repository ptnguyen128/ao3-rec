{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "from unidecode import unidecode\n",
    "\n",
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_empty = False\n",
    "username = \"incessantbeat\"\n",
    "url = f'https://archiveofourown.org/users/{username}/bookmarks'\n",
    "page = 1\n",
    "num_requested_fic = 0\n",
    "num_recorded_fic = 0\n",
    "csv_name = \"\"\n",
    "multichap_only = False\n",
    "tags = []\n",
    "\n",
    "# keep track of all processed ids to avoid repeats:\n",
    "# this is separate from the temporary batch of ids\n",
    "# that are written to the csv and then forgotten\n",
    "seen_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main user's bookmarked ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids():\n",
    "    \"\"\" Get work ids of bookmarked fics \"\"\"\n",
    "    global page_empty\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, \"lxml\")\n",
    "\n",
    "    fics = soup.select(\"li.bookmark.blurb.group\")\n",
    "    # see if we've gone too far and run out of fic:\n",
    "    if len(fics) == 0:\n",
    "        page_empty = True\n",
    "        print(\"No more fics to fetch!\")\n",
    "        return\n",
    "\n",
    "    # process list for new fic ids\n",
    "    ids = []\n",
    "    for idx, f in enumerate(fics):\n",
    "        try:\n",
    "            header = f.find('h4', class_='heading').find(href=True)\n",
    "            t = header['href'].split('/')[-1]\n",
    "            n = header.text\n",
    "            if t not in seen_ids:\n",
    "                ids.append(t)\n",
    "                seen_ids.append(t)\n",
    "        except:\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "def update_url_to_next_page():\n",
    "    global url\n",
    "    global page\n",
    "    key = \"page=\"\n",
    "    start = url.find(key)\n",
    "\n",
    "    # there is already a page indicator in the url\n",
    "    if start != -1:\n",
    "        # find where in the url the page indicator starts and ends\n",
    "        page_start_index = start + len(key)\n",
    "        page_end_index = url.find(\"&\", page_start_index)\n",
    "        # if it's in the middle of the url\n",
    "        if page_end_index != -1:\n",
    "            page = int(url[page_start_index:page_end_index]) + 1\n",
    "            url = url[:page_start_index] + str(page) + url[page_end_index:]\n",
    "        # if it's at the end of the url\n",
    "        else:\n",
    "            page = int(url[page_start_index:]) + 1\n",
    "            url = url[:page_start_index] + str(page)\n",
    "\n",
    "    # there is no page indicator, so we are on page 1\n",
    "    else:\n",
    "        # there are other modifiers\n",
    "        if url.find(\"?\") != -1:\n",
    "            url = url + \"&page=2\"\n",
    "        # there are no modifiers yet\n",
    "        else:\n",
    "            url = url + \"?page=2\"\n",
    "        page = 2\n",
    "\n",
    "\n",
    "def retrieve_ids():\n",
    "    while not page_empty:\n",
    "        time.sleep(5)\n",
    "        print(f\"Processing page {page}...\")\n",
    "        ids = get_ids()\n",
    "        update_url_to_next_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1...\n",
      "Processing page 2...\n",
      "No more fics to fetch!\n",
      "['28429821', '33658459', '33494707', '29839875', '21332461', '25333126', '26027836', '29618487', '29288772', '25839787', '23586082', '24363613', '25681186', '23492518', '24260716', '12830118', '12520952']\n"
     ]
    }
   ],
   "source": [
    "retrieve_ids()\n",
    "print(seen_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: do I need to write bookmarked ids to a text file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata of bookmarked ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get author(s)\n",
    "def get_authors(meta):\n",
    "    tags = meta.contents\n",
    "    authors = []\n",
    "    for tag in tags:\n",
    "        if tag.name == 'a':\n",
    "            authors.append(tag.contents[0])\n",
    "    return authors\n",
    "\n",
    "def get_tag_info(category, meta):\n",
    "    '''\n",
    "    given a category and a 'work meta group, returns a list of tags (eg, 'rating' -> 'explicit')\n",
    "    '''\n",
    "    try:\n",
    "        tag_list = meta.find(\"dd\", class_=str(category) + ' tags').find_all(class_=\"tag\")\n",
    "    except AttributeError as e:\n",
    "        return []\n",
    "    return [unidecode(result.text).rstrip().lstrip().lower() for result in tag_list] \n",
    "\n",
    "def get_tags(meta):\n",
    "    '''\n",
    "    returns a list of lists, of\n",
    "    rating, category, fandom, pairing, characters, additional_tags\n",
    "    '''\n",
    "    tags = ['rating', 'category', 'fandom', 'relationship', 'character', 'freeform']\n",
    "    info_list = list(map(lambda tag: get_tag_info(tag, meta), tags))\n",
    "    res = {}\n",
    "    for tag, info in zip(tags, info_list):\n",
    "        res[tag] = info\n",
    "    return res\n",
    "\n",
    "def get_stats(meta):\n",
    "    categories = ['language', 'published', 'status', 'words', 'chapters', 'comments', 'kudos', 'bookmarks', 'hits']\n",
    "    stats = list(map(lambda category: meta.find(\"dd\", class_=category), categories))\n",
    "    res = {}\n",
    "    for cat, stat in zip(categories, stats):\n",
    "        if stat:\n",
    "            res[cat] = unidecode(stat.text).rstrip().lstrip().lower()\n",
    "        else:\n",
    "            res[cat] = np.nan\n",
    "    return res\n",
    "\n",
    "def get_kudos(meta):\n",
    "    if (meta):\n",
    "        users = []\n",
    "        ## hunt for kudos' contents\n",
    "        kudos = meta.contents\n",
    "\n",
    "        # extract user names\n",
    "        for kudo in kudos:\n",
    "            if kudo.name == 'a':\n",
    "                if 'more users' not in kudo.contents[0] and '(collapse)' not in kudo.contents[0]:\n",
    "                    users.append(kudo.contents[0])\n",
    "\n",
    "        return users\n",
    "    return []\n",
    "\n",
    "# get bookmarks by page\n",
    "def get_bookmarks(url):\n",
    "    bookmarks = []\n",
    "\n",
    "    req = requests.get(url)\n",
    "    src = req.text\n",
    "\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(src, 'html.parser')\n",
    "\n",
    "    print('scraping bookmarks ')\n",
    "\n",
    "    # find all pages\n",
    "    if (soup.find('ol', class_='pagination actions')):\n",
    "        pages = soup.find('ol', class_='pagination actions').findChildren(\"li\" , recursive=False)\n",
    "        max_pages = int(pages[-2].contents[0].contents[0])\n",
    "        count = 1\n",
    "\n",
    "        while count <= max_pages:\n",
    "            # extract each bookmark per user\n",
    "            tags = soup.findAll('h5', class_='byline heading')\n",
    "            bookmarks += get_other_users(tags)\n",
    "\n",
    "            # next page\n",
    "            count+=1\n",
    "            req = requests.get(url+'?page='+str(count))\n",
    "            src = req.text\n",
    "            soup = BeautifulSoup(src, 'html.parser')\n",
    "            time.sleep(5)\n",
    "    else:\n",
    "        tags = soup.findAll('h5', class_='byline heading')\n",
    "        bookmarks += get_other_users(tags)\n",
    "    return bookmarks\n",
    "\n",
    "# get users form bookmarks, excluding yourself\n",
    "def get_other_users (meta):\n",
    "    users = []\n",
    "    for tag in meta:\n",
    "            user = tag.findChildren(\"a\" , recursive=False)[0].contents[0]\n",
    "            users.append(user)\n",
    "    return [u for u in users if u != username]\n",
    "\n",
    "def access_denied(soup):\n",
    "    if soup.find(class_=\"flash error\"):\n",
    "        return True\n",
    "    if not soup.find(class_=\"work meta group\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_metadata(fic_id):\n",
    "    print(f\"Scraping {fic_id}...\")\n",
    "    url = f'http://archiveofourown.org/works/{fic_id}?view_adult=true'\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    if access_denied(soup):\n",
    "        print('Access Denied')\n",
    "        return\n",
    "    else:\n",
    "        meta = soup.find(\"dl\", class_=\"work meta group\")\n",
    "        meta_dict = {}\n",
    "        meta_dict[\"url\"] = url\n",
    "        meta_dict[\"author\"] = get_authors(soup.find(\"h3\", class_=\"byline heading\"))\n",
    "        # unpack tags and stats sub-tags\n",
    "        for key, value in {**get_tags(meta), **get_stats(meta)}.items():\n",
    "            if len(value) > 1:\n",
    "                meta_dict[key] = value\n",
    "            else:\n",
    "                meta_dict[key] = value[0]\n",
    "        meta_dict[\"title\"] = unidecode(soup.find(\"h2\", class_=\"title heading\").string).strip()\n",
    "        visible_kudos = get_kudos(soup.find('p', class_='kudos'))\n",
    "        hidden_kudos = get_kudos(soup.find('span', class_='kudos_expanded hidden'))\n",
    "        meta_dict[\"all_kudos\"] = visible_kudos + hidden_kudos\n",
    "        \n",
    "        #get bookmarks\n",
    "        bookmark_url = f'http://archiveofourown.org/works/{fic_id}/bookmarks'\n",
    "        meta_dict[\"all_bookmarks\"] = get_bookmarks(bookmark_url)\n",
    "        return meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('rating', ['mature']), ('category', ['m/m']), ('fandom', ['nct (band)']), ('relationship', ['lee donghyuck | haechan/mark lee']), ('character', ['mark lee (nct)', 'lee donghyuck | haechan', 'na jaemin']), ('freeform', ['alternate universe - college/university', 'friends with benefits', 'making out', 'alcohol and weed', 'angst', 'unrequited crush', 'pining', 'drunken confessions', 'drunken kissing', 'markhyuck week 2021', 'day 5: touch | passion', 'sexual tension', 'unresolved romantic tension']), ('language', 'english'), ('published', '2020-12-30'), ('status', nan), ('words', '4199'), ('chapters', '1/1'), ('comments', '32'), ('kudos', '369'), ('bookmarks', '42'), ('hits', '4669')])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f'http://archiveofourown.org/works/28429821?view_adult=true'\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "meta = soup.find(\"dl\", class_=\"work meta group\")\n",
    "dest = {**get_tags(meta), **get_stats(meta)} \n",
    "dest.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 28429821...\n",
      "scraping bookmarks \n"
     ]
    }
   ],
   "source": [
    "d = get_metadata(28429821)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'http://archiveofourown.org/works/28429821?view_adult=true',\n",
       " 'author': ['tinymark (lumoon33)'],\n",
       " 'tags': {'rating': ['mature'],\n",
       "  'category': ['m/m'],\n",
       "  'fandom': ['nct (band)'],\n",
       "  'relationship': ['lee donghyuck | haechan/mark lee'],\n",
       "  'character': ['mark lee (nct)', 'lee donghyuck | haechan', 'na jaemin'],\n",
       "  'freeform': ['alternate universe - college/university',\n",
       "   'friends with benefits',\n",
       "   'making out',\n",
       "   'alcohol and weed',\n",
       "   'angst',\n",
       "   'unrequited crush',\n",
       "   'pining',\n",
       "   'drunken confessions',\n",
       "   'drunken kissing',\n",
       "   'markhyuck week 2021',\n",
       "   'day 5: touch | passion',\n",
       "   'sexual tension',\n",
       "   'unresolved romantic tension']},\n",
       " 'stats': {'language': 'english',\n",
       "  'published': '2020-12-30',\n",
       "  'status': nan,\n",
       "  'words': '4199',\n",
       "  'chapters': '1/1',\n",
       "  'comments': '32',\n",
       "  'kudos': '369',\n",
       "  'bookmarks': '42',\n",
       "  'hits': '4669'},\n",
       " 'title': 'falling to the bathroom floor',\n",
       " 'all_kudos': ['otterseoul',\n",
       "  'ari_thereyet',\n",
       "  'Slut_Seokjinnie',\n",
       "  'skymoonlight',\n",
       "  'nomdeguerre',\n",
       "  'haechlovebot',\n",
       "  'leehaechanace',\n",
       "  'a_random_parsnip',\n",
       "  'beom00',\n",
       "  'al1track2',\n",
       "  'jeonghanmicasa',\n",
       "  'afterjun',\n",
       "  'Loonaelove',\n",
       "  'whaelan',\n",
       "  'iaaaaane',\n",
       "  'awkwardsmiles',\n",
       "  'donotwonder',\n",
       "  'Artemisian11',\n",
       "  'mnunes_2112',\n",
       "  'silversunsetz',\n",
       "  'renheis',\n",
       "  'Niceskirtchan',\n",
       "  'Steffnra',\n",
       "  'renjunlovebug',\n",
       "  'snapchan',\n",
       "  'Luciddreamsj',\n",
       "  'seafaerie',\n",
       "  'enamors',\n",
       "  'pro_pwned',\n",
       "  'bynocturnes',\n",
       "  'lty_ldh',\n",
       "  'meilinjun',\n",
       "  'fenjc',\n",
       "  'clear_preferences',\n",
       "  'berylliuum',\n",
       "  'prkxjn',\n",
       "  'ksoogoth',\n",
       "  'tuliplyyy',\n",
       "  'Deanyeollie',\n",
       "  'devrhyuck',\n",
       "  'alias_lx',\n",
       "  'honeybunhyuck',\n",
       "  'yangkira',\n",
       "  'Wo0o0o',\n",
       "  'UGHFOREVERAIN',\n",
       "  'morio',\n",
       "  'selfdestruct',\n",
       "  'estiu',\n",
       "  'to_unexplain_the_unforgivable',\n",
       "  'heliotropism99'],\n",
       " 'all_bookmarks': ['chenlecentric',\n",
       "  'leehaechanace',\n",
       "  'beom00',\n",
       "  'lovedials',\n",
       "  'awkwardsmiles',\n",
       "  'deadrosexx',\n",
       "  'bynocturnes',\n",
       "  'gohchujang',\n",
       "  'st4rmanta',\n",
       "  'prkxjn',\n",
       "  'Zodiaclmao',\n",
       "  'tuliplyyy',\n",
       "  'UGHFOREVERAIN',\n",
       "  'mahaeism',\n",
       "  'yeorobun',\n",
       "  'LuigiLM',\n",
       "  'honestlybangtantrash',\n",
       "  'minhyun9s',\n",
       "  'moltenvintagelacedress',\n",
       "  'zojirushi',\n",
       "  'zojirushi',\n",
       "  'namiecakes',\n",
       "  'fullsunlouis',\n",
       "  'fullsunlouis',\n",
       "  'mahaesuperior',\n",
       "  'archerygod',\n",
       "  'jaehyuckist',\n",
       "  'beneathyourbravery',\n",
       "  'candyhyuck (candymark)',\n",
       "  'larravich',\n",
       "  'clovetree',\n",
       "  '107cm',\n",
       "  'EtheriaDragneel',\n",
       "  'sunflowerforfullsun',\n",
       "  'fullsundays',\n",
       "  'mahaezzz',\n",
       "  'aburn',\n",
       "  'laulliepolly',\n",
       "  'Cucky222011',\n",
       "  'Littlemarklee',\n",
       "  'broccolee_7']}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([d]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>category</th>\n",
       "      <th>fandom</th>\n",
       "      <th>relationship</th>\n",
       "      <th>character</th>\n",
       "      <th>freeform</th>\n",
       "      <th>language</th>\n",
       "      <th>...</th>\n",
       "      <th>status</th>\n",
       "      <th>words</th>\n",
       "      <th>chapters</th>\n",
       "      <th>comments</th>\n",
       "      <th>kudos</th>\n",
       "      <th>bookmarks</th>\n",
       "      <th>hits</th>\n",
       "      <th>title</th>\n",
       "      <th>all_kudos</th>\n",
       "      <th>all_bookmarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://archiveofourown.org/works/28429821?view...</td>\n",
       "      <td>[tinymark (lumoon33)]</td>\n",
       "      <td>[mature]</td>\n",
       "      <td>[m/m]</td>\n",
       "      <td>[nct (band)]</td>\n",
       "      <td>[lee donghyuck | haechan/mark lee]</td>\n",
       "      <td>[mark lee (nct), lee donghyuck | haechan, na j...</td>\n",
       "      <td>[alternate universe - college/university, frie...</td>\n",
       "      <td>english</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4199</td>\n",
       "      <td>1/1</td>\n",
       "      <td>32</td>\n",
       "      <td>369</td>\n",
       "      <td>42</td>\n",
       "      <td>4669</td>\n",
       "      <td>falling to the bathroom floor</td>\n",
       "      <td>[otterseoul, ari_thereyet, Slut_Seokjinnie, sk...</td>\n",
       "      <td>[chenlecentric, leehaechanace, beom00, lovedia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                url  \\\n",
       "0      0  http://archiveofourown.org/works/28429821?view...   \n",
       "\n",
       "                  author    rating category        fandom  \\\n",
       "0  [tinymark (lumoon33)]  [mature]    [m/m]  [nct (band)]   \n",
       "\n",
       "                         relationship  \\\n",
       "0  [lee donghyuck | haechan/mark lee]   \n",
       "\n",
       "                                           character  \\\n",
       "0  [mark lee (nct), lee donghyuck | haechan, na j...   \n",
       "\n",
       "                                            freeform language  ... status  \\\n",
       "0  [alternate universe - college/university, frie...  english  ...    NaN   \n",
       "\n",
       "   words chapters comments kudos bookmarks  hits  \\\n",
       "0   4199      1/1       32   369        42  4669   \n",
       "\n",
       "                           title  \\\n",
       "0  falling to the bathroom floor   \n",
       "\n",
       "                                           all_kudos  \\\n",
       "0  [otterseoul, ari_thereyet, Slut_Seokjinnie, sk...   \n",
       "\n",
       "                                       all_bookmarks  \n",
       "0  [chenlecentric, leehaechanace, beom00, lovedia...  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
